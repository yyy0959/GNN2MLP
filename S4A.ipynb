{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef854fbc-9b50-4c9a-988c-49ae373f15b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.04696326330304146\n",
      "Epoch [20/100], Loss: 0.04212697595357895\n",
      "Epoch [30/100], Loss: 0.040298350155353546\n",
      "Epoch [40/100], Loss: 0.039844438433647156\n",
      "Epoch [50/100], Loss: 0.0396617129445076\n",
      "Epoch [60/100], Loss: 0.03959028050303459\n",
      "Epoch [70/100], Loss: 0.039566151797771454\n",
      "Epoch [80/100], Loss: 0.039556071162223816\n",
      "Epoch [90/100], Loss: 0.03955245018005371\n",
      "Epoch [100/100], Loss: 0.0395512618124485\n",
      "Epoch: 001, Loss: 2.0926, Train Acc: 0.2429, Val Acc: 0.1780, Test Acc: 0.1520\n",
      "Epoch: 002, Loss: 1.8350, Train Acc: 0.3643, Val Acc: 0.2640, Test Acc: 0.2610\n",
      "Epoch: 003, Loss: 1.6551, Train Acc: 0.5500, Val Acc: 0.3440, Test Acc: 0.3450\n",
      "Epoch: 004, Loss: 1.3951, Train Acc: 0.6143, Val Acc: 0.4500, Test Acc: 0.4570\n",
      "Epoch: 005, Loss: 1.4508, Train Acc: 0.6786, Val Acc: 0.5320, Test Acc: 0.5560\n",
      "Epoch: 006, Loss: 1.3336, Train Acc: 0.6929, Val Acc: 0.5540, Test Acc: 0.5950\n",
      "Epoch: 007, Loss: 1.2995, Train Acc: 0.6857, Val Acc: 0.5560, Test Acc: 0.5970\n",
      "Epoch: 008, Loss: 1.1134, Train Acc: 0.6786, Val Acc: 0.5720, Test Acc: 0.5950\n",
      "Epoch: 009, Loss: 1.3341, Train Acc: 0.7071, Val Acc: 0.5880, Test Acc: 0.6010\n",
      "Epoch: 010, Loss: 1.1692, Train Acc: 0.7286, Val Acc: 0.5820, Test Acc: 0.5960\n",
      "Epoch: 011, Loss: 1.1927, Train Acc: 0.7786, Val Acc: 0.5580, Test Acc: 0.5830\n",
      "Epoch: 012, Loss: 1.2652, Train Acc: 0.7857, Val Acc: 0.5560, Test Acc: 0.5850\n",
      "Epoch: 013, Loss: 1.0430, Train Acc: 0.7857, Val Acc: 0.5540, Test Acc: 0.5900\n",
      "Epoch: 014, Loss: 1.0588, Train Acc: 0.7643, Val Acc: 0.5540, Test Acc: 0.5970\n",
      "Epoch: 015, Loss: 1.0807, Train Acc: 0.7857, Val Acc: 0.5540, Test Acc: 0.5970\n",
      "Epoch: 016, Loss: 1.2381, Train Acc: 0.7786, Val Acc: 0.5580, Test Acc: 0.6030\n",
      "Epoch: 017, Loss: 1.0979, Train Acc: 0.7714, Val Acc: 0.5760, Test Acc: 0.6070\n",
      "Epoch: 018, Loss: 0.9288, Train Acc: 0.7786, Val Acc: 0.5620, Test Acc: 0.6060\n",
      "Epoch: 019, Loss: 1.0130, Train Acc: 0.7857, Val Acc: 0.5640, Test Acc: 0.6040\n",
      "Epoch: 020, Loss: 1.0826, Train Acc: 0.8071, Val Acc: 0.5560, Test Acc: 0.6080\n",
      "Epoch: 021, Loss: 1.0862, Train Acc: 0.8071, Val Acc: 0.5600, Test Acc: 0.6120\n",
      "Epoch: 022, Loss: 1.1163, Train Acc: 0.8214, Val Acc: 0.5740, Test Acc: 0.6200\n",
      "Epoch: 023, Loss: 0.8574, Train Acc: 0.8214, Val Acc: 0.5820, Test Acc: 0.6130\n",
      "Epoch: 024, Loss: 0.9643, Train Acc: 0.8357, Val Acc: 0.5700, Test Acc: 0.6120\n",
      "Epoch: 025, Loss: 0.9402, Train Acc: 0.8286, Val Acc: 0.5640, Test Acc: 0.6050\n",
      "Epoch: 026, Loss: 1.0657, Train Acc: 0.8286, Val Acc: 0.5620, Test Acc: 0.6040\n",
      "Epoch: 027, Loss: 1.0359, Train Acc: 0.8357, Val Acc: 0.5600, Test Acc: 0.6040\n",
      "Epoch: 028, Loss: 1.0226, Train Acc: 0.8429, Val Acc: 0.5520, Test Acc: 0.6060\n",
      "Epoch: 029, Loss: 0.8449, Train Acc: 0.8357, Val Acc: 0.5540, Test Acc: 0.6090\n",
      "Epoch: 030, Loss: 0.8511, Train Acc: 0.8286, Val Acc: 0.5540, Test Acc: 0.6030\n",
      "Epoch: 031, Loss: 0.9379, Train Acc: 0.8357, Val Acc: 0.5520, Test Acc: 0.6040\n",
      "Epoch: 032, Loss: 0.8056, Train Acc: 0.8429, Val Acc: 0.5500, Test Acc: 0.6080\n",
      "Epoch: 033, Loss: 0.9127, Train Acc: 0.8429, Val Acc: 0.5540, Test Acc: 0.6090\n",
      "Epoch: 034, Loss: 0.9229, Train Acc: 0.8571, Val Acc: 0.5500, Test Acc: 0.6090\n",
      "Epoch: 035, Loss: 0.9194, Train Acc: 0.8571, Val Acc: 0.5580, Test Acc: 0.6180\n",
      "Epoch: 036, Loss: 0.8644, Train Acc: 0.8643, Val Acc: 0.5720, Test Acc: 0.6120\n",
      "Epoch: 037, Loss: 0.9796, Train Acc: 0.8786, Val Acc: 0.5780, Test Acc: 0.6090\n",
      "Epoch: 038, Loss: 0.8408, Train Acc: 0.8786, Val Acc: 0.5840, Test Acc: 0.6080\n",
      "Epoch: 039, Loss: 0.8154, Train Acc: 0.8929, Val Acc: 0.5820, Test Acc: 0.6180\n",
      "Epoch: 040, Loss: 0.8134, Train Acc: 0.8929, Val Acc: 0.5820, Test Acc: 0.6170\n",
      "Epoch: 041, Loss: 0.8822, Train Acc: 0.8929, Val Acc: 0.5780, Test Acc: 0.6190\n",
      "Epoch: 042, Loss: 0.7629, Train Acc: 0.8714, Val Acc: 0.5640, Test Acc: 0.6080\n",
      "Epoch: 043, Loss: 0.9144, Train Acc: 0.8714, Val Acc: 0.5620, Test Acc: 0.6030\n",
      "Epoch: 044, Loss: 0.8138, Train Acc: 0.8786, Val Acc: 0.5580, Test Acc: 0.6070\n",
      "Epoch: 045, Loss: 0.7429, Train Acc: 0.8929, Val Acc: 0.5600, Test Acc: 0.6030\n",
      "Epoch: 046, Loss: 0.6889, Train Acc: 0.9000, Val Acc: 0.5640, Test Acc: 0.6010\n",
      "Epoch: 047, Loss: 0.7489, Train Acc: 0.8929, Val Acc: 0.5600, Test Acc: 0.6030\n",
      "Epoch: 048, Loss: 0.8089, Train Acc: 0.8857, Val Acc: 0.5520, Test Acc: 0.6050\n",
      "Epoch: 049, Loss: 0.7934, Train Acc: 0.8714, Val Acc: 0.5400, Test Acc: 0.6030\n",
      "Epoch: 050, Loss: 0.7696, Train Acc: 0.8786, Val Acc: 0.5360, Test Acc: 0.5940\n",
      "Epoch: 051, Loss: 0.7355, Train Acc: 0.8714, Val Acc: 0.5380, Test Acc: 0.5780\n",
      "Epoch: 052, Loss: 0.8097, Train Acc: 0.8714, Val Acc: 0.5420, Test Acc: 0.5780\n",
      "Epoch: 053, Loss: 0.7928, Train Acc: 0.8786, Val Acc: 0.5500, Test Acc: 0.5730\n",
      "Epoch: 054, Loss: 0.7478, Train Acc: 0.8714, Val Acc: 0.5560, Test Acc: 0.5810\n",
      "Epoch: 055, Loss: 0.7847, Train Acc: 0.8714, Val Acc: 0.5580, Test Acc: 0.5870\n",
      "Epoch: 056, Loss: 0.7595, Train Acc: 0.8571, Val Acc: 0.5640, Test Acc: 0.6020\n",
      "Epoch: 057, Loss: 0.7032, Train Acc: 0.8571, Val Acc: 0.5720, Test Acc: 0.6050\n",
      "Epoch: 058, Loss: 0.8888, Train Acc: 0.8786, Val Acc: 0.5840, Test Acc: 0.6140\n",
      "Epoch: 059, Loss: 0.8212, Train Acc: 0.8786, Val Acc: 0.5860, Test Acc: 0.6160\n",
      "Epoch: 060, Loss: 0.6958, Train Acc: 0.8929, Val Acc: 0.5720, Test Acc: 0.5970\n",
      "Epoch: 061, Loss: 0.7347, Train Acc: 0.9000, Val Acc: 0.5680, Test Acc: 0.5790\n",
      "Epoch: 062, Loss: 0.6946, Train Acc: 0.9143, Val Acc: 0.5660, Test Acc: 0.5790\n",
      "Epoch: 063, Loss: 0.6958, Train Acc: 0.9143, Val Acc: 0.5540, Test Acc: 0.5720\n",
      "Epoch: 064, Loss: 0.8419, Train Acc: 0.9071, Val Acc: 0.5580, Test Acc: 0.5780\n",
      "Epoch: 065, Loss: 0.7535, Train Acc: 0.8857, Val Acc: 0.5720, Test Acc: 0.5810\n",
      "Epoch: 066, Loss: 0.7695, Train Acc: 0.8786, Val Acc: 0.5820, Test Acc: 0.5890\n",
      "Epoch: 067, Loss: 0.8472, Train Acc: 0.8929, Val Acc: 0.5800, Test Acc: 0.5930\n",
      "Epoch: 068, Loss: 0.7350, Train Acc: 0.8929, Val Acc: 0.5840, Test Acc: 0.6010\n",
      "Epoch: 069, Loss: 0.7921, Train Acc: 0.8857, Val Acc: 0.5860, Test Acc: 0.5980\n",
      "Epoch: 070, Loss: 0.8167, Train Acc: 0.8714, Val Acc: 0.5820, Test Acc: 0.6040\n",
      "Epoch: 071, Loss: 0.7442, Train Acc: 0.8786, Val Acc: 0.5840, Test Acc: 0.5900\n",
      "Epoch: 072, Loss: 0.7771, Train Acc: 0.8857, Val Acc: 0.5880, Test Acc: 0.5940\n",
      "Epoch: 073, Loss: 0.7819, Train Acc: 0.9071, Val Acc: 0.5880, Test Acc: 0.5920\n",
      "Epoch: 074, Loss: 0.7557, Train Acc: 0.9071, Val Acc: 0.5740, Test Acc: 0.5890\n",
      "Epoch: 075, Loss: 0.7651, Train Acc: 0.9000, Val Acc: 0.5740, Test Acc: 0.5880\n",
      "Epoch: 076, Loss: 0.6990, Train Acc: 0.8929, Val Acc: 0.5620, Test Acc: 0.5920\n",
      "Epoch: 077, Loss: 0.9550, Train Acc: 0.8929, Val Acc: 0.5740, Test Acc: 0.6030\n",
      "Epoch: 078, Loss: 0.6292, Train Acc: 0.9000, Val Acc: 0.5720, Test Acc: 0.6110\n",
      "Epoch: 079, Loss: 0.7599, Train Acc: 0.9000, Val Acc: 0.5740, Test Acc: 0.6060\n",
      "Epoch: 080, Loss: 0.6650, Train Acc: 0.8929, Val Acc: 0.5760, Test Acc: 0.6110\n",
      "Epoch: 081, Loss: 0.6562, Train Acc: 0.8929, Val Acc: 0.5760, Test Acc: 0.6180\n",
      "Epoch: 082, Loss: 0.8406, Train Acc: 0.8929, Val Acc: 0.5820, Test Acc: 0.6230\n",
      "Epoch: 083, Loss: 0.6578, Train Acc: 0.8929, Val Acc: 0.5820, Test Acc: 0.6180\n",
      "Epoch: 084, Loss: 0.6577, Train Acc: 0.9143, Val Acc: 0.5900, Test Acc: 0.6190\n",
      "Epoch: 085, Loss: 0.5930, Train Acc: 0.9143, Val Acc: 0.5940, Test Acc: 0.6170\n",
      "Epoch: 086, Loss: 0.9046, Train Acc: 0.9357, Val Acc: 0.6000, Test Acc: 0.6210\n",
      "Epoch: 087, Loss: 0.8284, Train Acc: 0.9357, Val Acc: 0.6040, Test Acc: 0.6250\n",
      "Epoch: 088, Loss: 0.8420, Train Acc: 0.9286, Val Acc: 0.5920, Test Acc: 0.6130\n",
      "Epoch: 089, Loss: 0.6246, Train Acc: 0.9071, Val Acc: 0.5860, Test Acc: 0.5990\n",
      "Epoch: 090, Loss: 0.7019, Train Acc: 0.8929, Val Acc: 0.5800, Test Acc: 0.5930\n",
      "Epoch: 091, Loss: 0.8002, Train Acc: 0.8786, Val Acc: 0.5760, Test Acc: 0.5950\n",
      "Epoch: 092, Loss: 0.8346, Train Acc: 0.8786, Val Acc: 0.5600, Test Acc: 0.5990\n",
      "Epoch: 093, Loss: 0.7030, Train Acc: 0.8786, Val Acc: 0.5660, Test Acc: 0.5950\n",
      "Epoch: 094, Loss: 0.7518, Train Acc: 0.8929, Val Acc: 0.5660, Test Acc: 0.5960\n",
      "Epoch: 095, Loss: 0.7847, Train Acc: 0.9071, Val Acc: 0.5680, Test Acc: 0.6040\n",
      "Epoch: 096, Loss: 0.6582, Train Acc: 0.9143, Val Acc: 0.5720, Test Acc: 0.6090\n",
      "Epoch: 097, Loss: 0.7321, Train Acc: 0.9143, Val Acc: 0.5740, Test Acc: 0.6200\n",
      "Epoch: 098, Loss: 0.6660, Train Acc: 0.9143, Val Acc: 0.5980, Test Acc: 0.6230\n",
      "Epoch: 099, Loss: 0.7958, Train Acc: 0.9071, Val Acc: 0.5800, Test Acc: 0.6110\n",
      "Epoch: 100, Loss: 0.6044, Train Acc: 0.9000, Val Acc: 0.5720, Test Acc: 0.6050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from torch_geometric.utils import remove_self_loops, degree\n",
    "\n",
    "# 加载Cora数据集\n",
    "dataset = Planetoid(root='/tmp/cora', name='cora')\n",
    "data = dataset[0]\n",
    "# 设定降维后的目标维度\n",
    "pca_dim = 32  # 你可以根据需要调整PCA降维后的维度\n",
    "\n",
    "# 对节点特征进行PCA降维\n",
    "def apply_pca(features, pca_dim):\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    pca_result = pca.fit_transform(features)\n",
    "    return torch.tensor(pca_result, dtype=torch.float)\n",
    "\n",
    "# # 将数据中的节点特征进行PCA降维\n",
    "data.x = apply_pca(data.x.numpy(), pca_dim)\n",
    "\n",
    "def gcn_conv(h, edge_index):\n",
    "    N = h.size(0)\n",
    "    edge_index, _ = remove_self_loops(edge_index)\n",
    "    \n",
    "    src, dst = edge_index\n",
    "    deg = degree(dst, num_nodes=N)\n",
    "    \n",
    "    deg_src = deg[src].pow(-0.5) \n",
    "    deg_src.masked_fill_(deg_src == float('inf'), 0)\n",
    "    deg_dst = deg[dst].pow(-0.5)\n",
    "    deg_dst.masked_fill_(deg_dst == float('inf'), 0)\n",
    "    edge_weight = deg_src * deg_dst\n",
    "\n",
    "    a = torch.sparse_coo_tensor(edge_index, edge_weight, torch.Size([N, N])).t()\n",
    "    h_prime = a @ h \n",
    "    return h_prime\n",
    "\n",
    "# 2. Define a simple neural network for estimating W\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "x = data.x\n",
    "edge_index = data.edge_index\n",
    "\n",
    "x_prime = gcn_conv(x, edge_index)\n",
    "x_prime = x_prime\n",
    "\n",
    "# 5. Prepare data for training\n",
    "input_features = x.size(1)\n",
    "output_features = x_prime.size(1)\n",
    "model = SimpleNN(input_features, output_features)\n",
    "\n",
    "# Use Mean Squared Error as the loss function\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# 6. Train the model with original x'\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_prime_pred = model(x)\n",
    "    loss = criterion(x_prime_pred, x_prime)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "data.x = model(data.x).detach()\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 添加Dropout层\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        hidden_1 = F.relu(self.bn1(self.fc1(x)))\n",
    "        hidden_1 = self.dropout(hidden_1)  # 在隐藏层之后应用Dropout\n",
    "        hidden_2 = F.relu(self.bn2(self.fc2(hidden_1)))\n",
    "        hidden_2 = self.dropout(hidden_2)\n",
    "        output = self.out(hidden_2)\n",
    "        return F.log_softmax(output, dim=1), hidden_1, hidden_2\n",
    "\n",
    "# 定义模型参数\n",
    "input_dim = data.x.shape[1]  # Cora数据集的输入特征维度\n",
    "hidden_dim = 32                        # 隐藏层维度为16\n",
    "output_dim = dataset.num_classes        # 分类数\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model2 = MLP(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.1, weight_decay=0.001)\n",
    "\n",
    "# 训练过程\n",
    "def train():\n",
    "    model2.train()\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    out, _, _  = model2(data)  # 前向传播\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "    return loss.item()\n",
    "\n",
    "# 测试过程\n",
    "def test():\n",
    "    model2.eval()\n",
    "    logits, accs = model2(data)[0], []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "# 训练和测试模型\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d9dd5-f3b3-44af-b5e5-7f6a1a8ebc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
