{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecc72f8-9b83-42f2-83c0-9cfd173ffd16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch 0, Teacher Model Accuracy: 0.3220\n",
      "Epoch 1, Teacher Model Accuracy: 0.4070\n",
      "Epoch 2, Teacher Model Accuracy: 0.4470\n",
      "Epoch 3, Teacher Model Accuracy: 0.4730\n",
      "Epoch 4, Teacher Model Accuracy: 0.4810\n",
      "Epoch 5, Teacher Model Accuracy: 0.5010\n",
      "Epoch 6, Teacher Model Accuracy: 0.5310\n",
      "Epoch 7, Teacher Model Accuracy: 0.5570\n",
      "Epoch 8, Teacher Model Accuracy: 0.5920\n",
      "Epoch 9, Teacher Model Accuracy: 0.6310\n",
      "Epoch 10, Teacher Model Accuracy: 0.6640\n",
      "Epoch 11, Teacher Model Accuracy: 0.6970\n",
      "Epoch 12, Teacher Model Accuracy: 0.7160\n",
      "Epoch 13, Teacher Model Accuracy: 0.7350\n",
      "Epoch 14, Teacher Model Accuracy: 0.7490\n",
      "Epoch 15, Teacher Model Accuracy: 0.7610\n",
      "Epoch 16, Teacher Model Accuracy: 0.7780\n",
      "Epoch 17, Teacher Model Accuracy: 0.7860\n",
      "Epoch 18, Teacher Model Accuracy: 0.7890\n",
      "Epoch 19, Teacher Model Accuracy: 0.7970\n",
      "Epoch 20, Teacher Model Accuracy: 0.8010\n",
      "Epoch 21, Teacher Model Accuracy: 0.8070\n",
      "Epoch 22, Teacher Model Accuracy: 0.8100\n",
      "Epoch 23, Teacher Model Accuracy: 0.8130\n",
      "Epoch 24, Teacher Model Accuracy: 0.8120\n",
      "Epoch 25, Teacher Model Accuracy: 0.8150\n",
      "Epoch 26, Teacher Model Accuracy: 0.8130\n",
      "Epoch 27, Teacher Model Accuracy: 0.8170\n",
      "Epoch 28, Teacher Model Accuracy: 0.8170\n",
      "Epoch 29, Teacher Model Accuracy: 0.8160\n",
      "Epoch 30, Teacher Model Accuracy: 0.8170\n",
      "Epoch 31, Teacher Model Accuracy: 0.8160\n",
      "Epoch 32, Teacher Model Accuracy: 0.8200\n",
      "Epoch 33, Teacher Model Accuracy: 0.8210\n",
      "Epoch 34, Teacher Model Accuracy: 0.8210\n",
      "Epoch 35, Teacher Model Accuracy: 0.8160\n",
      "Epoch 36, Teacher Model Accuracy: 0.8160\n",
      "Epoch 37, Teacher Model Accuracy: 0.8150\n",
      "Epoch 38, Teacher Model Accuracy: 0.8150\n",
      "Epoch 39, Teacher Model Accuracy: 0.8140\n",
      "Epoch 40, Teacher Model Accuracy: 0.8140\n",
      "Epoch 41, Teacher Model Accuracy: 0.8130\n",
      "Epoch 42, Teacher Model Accuracy: 0.8130\n",
      "Epoch 43, Teacher Model Accuracy: 0.8120\n",
      "Epoch 44, Teacher Model Accuracy: 0.8120\n",
      "Epoch 45, Teacher Model Accuracy: 0.8100\n",
      "Epoch 46, Teacher Model Accuracy: 0.8110\n",
      "Epoch 47, Teacher Model Accuracy: 0.8130\n",
      "Epoch 48, Teacher Model Accuracy: 0.8150\n",
      "Epoch 49, Teacher Model Accuracy: 0.8160\n",
      "Epoch 50, Teacher Model Accuracy: 0.8150\n",
      "Epoch 51, Teacher Model Accuracy: 0.8140\n",
      "Epoch 52, Teacher Model Accuracy: 0.8140\n",
      "Epoch 53, Teacher Model Accuracy: 0.8130\n",
      "Epoch 54, Teacher Model Accuracy: 0.8150\n",
      "Epoch 55, Teacher Model Accuracy: 0.8150\n",
      "Epoch 56, Teacher Model Accuracy: 0.8150\n",
      "Epoch 57, Teacher Model Accuracy: 0.8130\n",
      "Epoch 58, Teacher Model Accuracy: 0.8130\n",
      "Epoch 59, Teacher Model Accuracy: 0.8130\n",
      "Epoch 60, Teacher Model Accuracy: 0.8140\n",
      "Epoch 61, Teacher Model Accuracy: 0.8150\n",
      "Epoch 62, Teacher Model Accuracy: 0.8150\n",
      "Epoch 63, Teacher Model Accuracy: 0.8150\n",
      "Epoch 64, Teacher Model Accuracy: 0.8150\n",
      "Epoch 65, Teacher Model Accuracy: 0.8150\n",
      "Epoch 66, Teacher Model Accuracy: 0.8150\n",
      "Epoch 67, Teacher Model Accuracy: 0.8150\n",
      "Epoch 68, Teacher Model Accuracy: 0.8150\n",
      "Epoch 69, Teacher Model Accuracy: 0.8150\n",
      "Epoch 70, Teacher Model Accuracy: 0.8150\n",
      "Epoch 71, Teacher Model Accuracy: 0.8150\n",
      "Epoch 72, Teacher Model Accuracy: 0.8160\n",
      "Epoch 73, Teacher Model Accuracy: 0.8160\n",
      "Epoch 74, Teacher Model Accuracy: 0.8150\n",
      "Epoch 75, Teacher Model Accuracy: 0.8160\n",
      "Epoch 76, Teacher Model Accuracy: 0.8170\n",
      "Epoch 77, Teacher Model Accuracy: 0.8180\n",
      "Epoch 78, Teacher Model Accuracy: 0.8180\n",
      "Epoch 79, Teacher Model Accuracy: 0.8190\n",
      "Epoch 80, Teacher Model Accuracy: 0.8190\n",
      "Epoch 81, Teacher Model Accuracy: 0.8190\n",
      "Epoch 82, Teacher Model Accuracy: 0.8180\n",
      "Epoch 83, Teacher Model Accuracy: 0.8200\n",
      "Epoch 84, Teacher Model Accuracy: 0.8200\n",
      "Epoch 85, Teacher Model Accuracy: 0.8200\n",
      "Epoch 86, Teacher Model Accuracy: 0.8200\n",
      "Epoch 87, Teacher Model Accuracy: 0.8200\n",
      "Epoch 88, Teacher Model Accuracy: 0.8200\n",
      "Epoch 89, Teacher Model Accuracy: 0.8210\n",
      "Epoch 90, Teacher Model Accuracy: 0.8210\n",
      "Epoch 91, Teacher Model Accuracy: 0.8210\n",
      "Epoch 92, Teacher Model Accuracy: 0.8210\n",
      "Epoch 93, Teacher Model Accuracy: 0.8210\n",
      "Epoch 94, Teacher Model Accuracy: 0.8210\n",
      "Epoch 95, Teacher Model Accuracy: 0.8210\n",
      "Epoch 96, Teacher Model Accuracy: 0.8190\n",
      "Epoch 97, Teacher Model Accuracy: 0.8200\n",
      "Epoch 98, Teacher Model Accuracy: 0.8200\n",
      "Epoch 99, Teacher Model Accuracy: 0.8200\n",
      "Epoch 0, Student Model Accuracy: 0.1300\n",
      "Epoch 1, Student Model Accuracy: 0.1300\n",
      "Epoch 2, Student Model Accuracy: 0.1300\n",
      "Epoch 3, Student Model Accuracy: 0.1340\n",
      "Epoch 4, Student Model Accuracy: 0.1650\n",
      "Epoch 5, Student Model Accuracy: 0.2190\n",
      "Epoch 6, Student Model Accuracy: 0.2690\n",
      "Epoch 7, Student Model Accuracy: 0.3460\n",
      "Epoch 8, Student Model Accuracy: 0.4070\n",
      "Epoch 9, Student Model Accuracy: 0.4550\n",
      "Epoch 10, Student Model Accuracy: 0.4640\n",
      "Epoch 11, Student Model Accuracy: 0.4780\n",
      "Epoch 12, Student Model Accuracy: 0.4850\n",
      "Epoch 13, Student Model Accuracy: 0.4900\n",
      "Epoch 14, Student Model Accuracy: 0.5020\n",
      "Epoch 15, Student Model Accuracy: 0.5220\n",
      "Epoch 16, Student Model Accuracy: 0.5250\n",
      "Epoch 17, Student Model Accuracy: 0.5380\n",
      "Epoch 18, Student Model Accuracy: 0.5300\n",
      "Epoch 19, Student Model Accuracy: 0.5370\n",
      "Epoch 20, Student Model Accuracy: 0.5340\n",
      "Epoch 21, Student Model Accuracy: 0.5330\n",
      "Epoch 22, Student Model Accuracy: 0.5430\n",
      "Epoch 23, Student Model Accuracy: 0.5380\n",
      "Epoch 24, Student Model Accuracy: 0.5410\n",
      "Epoch 25, Student Model Accuracy: 0.5370\n",
      "Epoch 26, Student Model Accuracy: 0.5470\n",
      "Epoch 27, Student Model Accuracy: 0.5440\n",
      "Epoch 28, Student Model Accuracy: 0.5490\n",
      "Epoch 29, Student Model Accuracy: 0.5290\n",
      "Epoch 30, Student Model Accuracy: 0.5510\n",
      "Epoch 31, Student Model Accuracy: 0.5600\n",
      "Epoch 32, Student Model Accuracy: 0.5680\n",
      "Epoch 33, Student Model Accuracy: 0.5870\n",
      "Epoch 34, Student Model Accuracy: 0.6040\n",
      "Epoch 35, Student Model Accuracy: 0.6130\n",
      "Epoch 36, Student Model Accuracy: 0.6180\n",
      "Epoch 37, Student Model Accuracy: 0.6180\n",
      "Epoch 38, Student Model Accuracy: 0.6210\n",
      "Epoch 39, Student Model Accuracy: 0.6220\n",
      "Epoch 40, Student Model Accuracy: 0.6300\n",
      "Epoch 41, Student Model Accuracy: 0.6320\n",
      "Epoch 42, Student Model Accuracy: 0.6390\n",
      "Epoch 43, Student Model Accuracy: 0.6450\n",
      "Epoch 44, Student Model Accuracy: 0.6570\n",
      "Epoch 45, Student Model Accuracy: 0.6590\n",
      "Epoch 46, Student Model Accuracy: 0.6650\n",
      "Epoch 47, Student Model Accuracy: 0.6740\n",
      "Epoch 48, Student Model Accuracy: 0.6800\n",
      "Epoch 49, Student Model Accuracy: 0.6890\n",
      "Epoch 50, Student Model Accuracy: 0.6950\n",
      "Epoch 51, Student Model Accuracy: 0.7000\n",
      "Epoch 52, Student Model Accuracy: 0.7040\n",
      "Epoch 53, Student Model Accuracy: 0.7040\n",
      "Epoch 54, Student Model Accuracy: 0.6980\n",
      "Epoch 55, Student Model Accuracy: 0.7010\n",
      "Epoch 56, Student Model Accuracy: 0.7080\n",
      "Epoch 57, Student Model Accuracy: 0.7100\n",
      "Epoch 58, Student Model Accuracy: 0.7150\n",
      "Epoch 59, Student Model Accuracy: 0.7170\n",
      "Epoch 60, Student Model Accuracy: 0.7280\n",
      "Epoch 61, Student Model Accuracy: 0.7190\n",
      "Epoch 62, Student Model Accuracy: 0.7250\n",
      "Epoch 63, Student Model Accuracy: 0.7270\n",
      "Epoch 64, Student Model Accuracy: 0.7260\n",
      "Epoch 65, Student Model Accuracy: 0.7240\n",
      "Epoch 66, Student Model Accuracy: 0.7280\n",
      "Epoch 67, Student Model Accuracy: 0.7260\n",
      "Epoch 68, Student Model Accuracy: 0.7270\n",
      "Epoch 69, Student Model Accuracy: 0.7230\n",
      "Epoch 70, Student Model Accuracy: 0.7300\n",
      "Epoch 71, Student Model Accuracy: 0.7360\n",
      "Epoch 72, Student Model Accuracy: 0.7310\n",
      "Epoch 73, Student Model Accuracy: 0.7390\n",
      "Epoch 74, Student Model Accuracy: 0.7390\n",
      "Epoch 75, Student Model Accuracy: 0.7400\n",
      "Epoch 76, Student Model Accuracy: 0.7390\n",
      "Epoch 77, Student Model Accuracy: 0.7380\n",
      "Epoch 78, Student Model Accuracy: 0.7410\n",
      "Epoch 79, Student Model Accuracy: 0.7350\n",
      "Epoch 80, Student Model Accuracy: 0.7360\n",
      "Epoch 81, Student Model Accuracy: 0.7340\n",
      "Epoch 82, Student Model Accuracy: 0.7420\n",
      "Epoch 83, Student Model Accuracy: 0.7310\n",
      "Epoch 84, Student Model Accuracy: 0.7490\n",
      "Epoch 85, Student Model Accuracy: 0.7410\n",
      "Epoch 86, Student Model Accuracy: 0.7430\n",
      "Epoch 87, Student Model Accuracy: 0.7480\n",
      "Epoch 88, Student Model Accuracy: 0.7500\n",
      "Epoch 89, Student Model Accuracy: 0.7500\n",
      "Epoch 90, Student Model Accuracy: 0.7370\n",
      "Epoch 91, Student Model Accuracy: 0.7510\n",
      "Epoch 92, Student Model Accuracy: 0.7490\n",
      "Epoch 93, Student Model Accuracy: 0.7520\n",
      "Epoch 94, Student Model Accuracy: 0.7460\n",
      "Epoch 95, Student Model Accuracy: 0.7450\n",
      "Epoch 96, Student Model Accuracy: 0.7450\n",
      "Epoch 97, Student Model Accuracy: 0.7530\n",
      "Epoch 98, Student Model Accuracy: 0.7460\n",
      "Epoch 99, Student Model Accuracy: 0.7490\n",
      "Epoch 100, Student Model Accuracy: 0.7500\n",
      "Epoch 101, Student Model Accuracy: 0.7510\n",
      "Epoch 102, Student Model Accuracy: 0.7570\n",
      "Epoch 103, Student Model Accuracy: 0.7520\n",
      "Epoch 104, Student Model Accuracy: 0.7530\n",
      "Epoch 105, Student Model Accuracy: 0.7490\n",
      "Epoch 106, Student Model Accuracy: 0.7570\n",
      "Epoch 107, Student Model Accuracy: 0.7500\n",
      "Epoch 108, Student Model Accuracy: 0.7530\n",
      "Epoch 109, Student Model Accuracy: 0.7420\n",
      "Epoch 110, Student Model Accuracy: 0.7560\n",
      "Epoch 111, Student Model Accuracy: 0.7560\n",
      "Epoch 112, Student Model Accuracy: 0.7520\n",
      "Epoch 113, Student Model Accuracy: 0.7590\n",
      "Epoch 114, Student Model Accuracy: 0.7510\n",
      "Epoch 115, Student Model Accuracy: 0.7560\n",
      "Epoch 116, Student Model Accuracy: 0.7600\n",
      "Epoch 117, Student Model Accuracy: 0.7480\n",
      "Epoch 118, Student Model Accuracy: 0.7600\n",
      "Epoch 119, Student Model Accuracy: 0.7530\n",
      "Epoch 120, Student Model Accuracy: 0.7560\n",
      "Epoch 121, Student Model Accuracy: 0.7600\n",
      "Epoch 122, Student Model Accuracy: 0.7580\n",
      "Epoch 123, Student Model Accuracy: 0.7630\n",
      "Epoch 124, Student Model Accuracy: 0.7530\n",
      "Epoch 125, Student Model Accuracy: 0.7640\n",
      "Epoch 126, Student Model Accuracy: 0.7550\n",
      "Epoch 127, Student Model Accuracy: 0.7610\n",
      "Epoch 128, Student Model Accuracy: 0.7620\n",
      "Epoch 129, Student Model Accuracy: 0.7590\n",
      "Epoch 130, Student Model Accuracy: 0.7640\n",
      "Epoch 131, Student Model Accuracy: 0.7590\n",
      "Epoch 132, Student Model Accuracy: 0.7600\n",
      "Epoch 133, Student Model Accuracy: 0.7560\n",
      "Epoch 134, Student Model Accuracy: 0.7640\n",
      "Epoch 135, Student Model Accuracy: 0.7600\n",
      "Epoch 136, Student Model Accuracy: 0.7670\n",
      "Epoch 137, Student Model Accuracy: 0.7640\n",
      "Epoch 138, Student Model Accuracy: 0.7610\n",
      "Epoch 139, Student Model Accuracy: 0.7620\n",
      "Epoch 140, Student Model Accuracy: 0.7560\n",
      "Epoch 141, Student Model Accuracy: 0.7680\n",
      "Epoch 142, Student Model Accuracy: 0.7570\n",
      "Epoch 143, Student Model Accuracy: 0.7660\n",
      "Epoch 144, Student Model Accuracy: 0.7600\n",
      "Epoch 145, Student Model Accuracy: 0.7670\n",
      "Epoch 146, Student Model Accuracy: 0.7700\n",
      "Epoch 147, Student Model Accuracy: 0.7570\n",
      "Epoch 148, Student Model Accuracy: 0.7610\n",
      "Epoch 149, Student Model Accuracy: 0.7630\n",
      "Epoch 150, Student Model Accuracy: 0.7650\n",
      "Epoch 151, Student Model Accuracy: 0.7580\n",
      "Epoch 152, Student Model Accuracy: 0.7660\n",
      "Epoch 153, Student Model Accuracy: 0.7520\n",
      "Epoch 154, Student Model Accuracy: 0.7580\n",
      "Epoch 155, Student Model Accuracy: 0.7620\n",
      "Epoch 156, Student Model Accuracy: 0.7610\n",
      "Epoch 157, Student Model Accuracy: 0.7630\n",
      "Epoch 158, Student Model Accuracy: 0.7610\n",
      "Epoch 159, Student Model Accuracy: 0.7630\n",
      "Epoch 160, Student Model Accuracy: 0.7610\n",
      "Epoch 161, Student Model Accuracy: 0.7640\n",
      "Epoch 162, Student Model Accuracy: 0.7590\n",
      "Epoch 163, Student Model Accuracy: 0.7710\n",
      "Epoch 164, Student Model Accuracy: 0.7730\n",
      "Epoch 165, Student Model Accuracy: 0.7540\n",
      "Epoch 166, Student Model Accuracy: 0.7710\n",
      "Epoch 167, Student Model Accuracy: 0.7540\n",
      "Epoch 168, Student Model Accuracy: 0.7660\n",
      "Epoch 169, Student Model Accuracy: 0.7670\n",
      "Epoch 170, Student Model Accuracy: 0.7680\n",
      "Epoch 171, Student Model Accuracy: 0.7650\n",
      "Epoch 172, Student Model Accuracy: 0.7690\n",
      "Epoch 173, Student Model Accuracy: 0.7640\n",
      "Epoch 174, Student Model Accuracy: 0.7680\n",
      "Epoch 175, Student Model Accuracy: 0.7620\n",
      "Epoch 176, Student Model Accuracy: 0.7610\n",
      "Epoch 177, Student Model Accuracy: 0.7600\n",
      "Epoch 178, Student Model Accuracy: 0.7690\n",
      "Epoch 179, Student Model Accuracy: 0.7540\n",
      "Epoch 180, Student Model Accuracy: 0.7690\n",
      "Epoch 181, Student Model Accuracy: 0.7560\n",
      "Epoch 182, Student Model Accuracy: 0.7700\n",
      "Epoch 183, Student Model Accuracy: 0.7730\n",
      "Epoch 184, Student Model Accuracy: 0.7630\n",
      "Epoch 185, Student Model Accuracy: 0.7700\n",
      "Epoch 186, Student Model Accuracy: 0.7610\n",
      "Epoch 187, Student Model Accuracy: 0.7710\n",
      "Epoch 188, Student Model Accuracy: 0.7690\n",
      "Epoch 189, Student Model Accuracy: 0.7750\n",
      "Epoch 190, Student Model Accuracy: 0.7740\n",
      "Epoch 191, Student Model Accuracy: 0.7640\n",
      "Epoch 192, Student Model Accuracy: 0.7770\n",
      "Epoch 193, Student Model Accuracy: 0.7590\n",
      "Epoch 194, Student Model Accuracy: 0.7750\n",
      "Epoch 195, Student Model Accuracy: 0.7720\n",
      "Epoch 196, Student Model Accuracy: 0.7700\n",
      "Epoch 197, Student Model Accuracy: 0.7700\n",
      "Epoch 198, Student Model Accuracy: 0.7560\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 195\u001b[0m\n\u001b[0;32m    192\u001b[0m     soft_labels \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(teacher_logits \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Temperature scaling\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Train the student with soft labels and ground truth\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m train_student(soft_labels)\n\u001b[0;32m    196\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate_student()\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Student Model Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 176\u001b[0m, in \u001b[0;36mtrain_student\u001b[1;34m(soft_labels)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Combined loss\u001b[39;00m\n\u001b[0;32m    175\u001b[0m loss \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m loss_soft \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m loss_hard \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m (layer_loss1 \u001b[38;5;241m+\u001b[39m layer_loss2) \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m orth_loss\n\u001b[1;32m--> 176\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    177\u001b[0m optimizer_student\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import remove_self_loops, degree\n",
    "\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "data = data.to(device)  # Move data to GPU\n",
    "\n",
    "def gcn_conv(h, edge_index):\n",
    "    N = h.size(0)\n",
    "    edge_index, _ = remove_self_loops(edge_index)\n",
    "    src, dst = edge_index\n",
    "    deg = degree(dst, num_nodes=N)\n",
    "    deg_src = deg[src].pow(-0.5) \n",
    "    deg_src.masked_fill_(deg_src == float('inf'), 0)\n",
    "    deg_dst = deg[dst].pow(-0.5)\n",
    "    deg_dst.masked_fill_(deg_dst == float('inf'), 0)\n",
    "    edge_weight = deg_src * deg_dst\n",
    "    a = torch.sparse_coo_tensor(edge_index, edge_weight, torch.Size([N, N])).t()\n",
    "    h_prime = a @ h \n",
    "    return h_prime\n",
    "\n",
    "\n",
    "def gcn_conv_low_filter(h, edge_index):\n",
    "    N = h.size(0)\n",
    "    edge_index, _ = remove_self_loops(edge_index)\n",
    "    src, dst = edge_index\n",
    "    deg = degree(dst, num_nodes=N)\n",
    "    deg_src = deg[src].pow(-0.5) \n",
    "    deg_src.masked_fill_(deg_src == float('inf'), 0)\n",
    "    deg_dst = deg[dst].pow(-0.5)\n",
    "    deg_dst.masked_fill_(deg_dst == float('inf'), 0)\n",
    "    edge_weight = deg_src * deg_dst\n",
    "    a = torch.sparse_coo_tensor(edge_index, edge_weight, torch.Size([N, N])).t()\n",
    "    i_matrix = torch.eye(N).to(h.device)\n",
    "    adj_matrix = i_matrix - a\n",
    "    h_prime = adj_matrix @ h\n",
    "    return h_prime\n",
    "\n",
    "# Define GCN (teacher) model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden_feats)\n",
    "        self.conv2 = GCNConv(hidden_feats, out_feats)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Define MLP (student) model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hidden_feats)\n",
    "        self.l1 = nn.Linear(hidden_feats, hidden_feats)\n",
    "        self.l1_low = nn.Linear(hidden_feats, hidden_feats)\n",
    "        self.fc2 = nn.Linear(hidden_feats, out_feats)\n",
    "        self.l2 = nn.Linear(out_feats, out_feats)\n",
    "        self.l2_low = nn.Linear(out_feats, out_feats)\n",
    "\n",
    "    def forward(self, data):\n",
    "        hidden_feat = []\n",
    "        x = data.x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        hidden_feat.append(x)\n",
    "        temp_x = x\n",
    "        x = self.l1(temp_x)\n",
    "        x_low = self.l1_low(temp_x)\n",
    "        hidden_feat.append(x)\n",
    "        hidden_feat.append(x_low)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        hidden_feat.append(x)\n",
    "        temp_x = x\n",
    "        x = self.l2(temp_x)\n",
    "        x_low = self.l2_low(temp_x)\n",
    "        hidden_feat.append(x)\n",
    "        hidden_feat.append(x_low)\n",
    "        return x, hidden_feat\n",
    "\n",
    "# Initialize models\n",
    "teacher_model = GCN(in_feats=dataset.num_features, hidden_feats=16, out_feats=dataset.num_classes).to(device)\n",
    "student_model = MLP(in_feats=dataset.num_features, hidden_feats=16, out_feats=dataset.num_classes).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_teacher = torch.optim.Adam(teacher_model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "optimizer_student = torch.optim.Adam(student_model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 100\n",
    "temperature = 2.0\n",
    "alpha = 0.7  # balance between soft target and true target\n",
    "beta = 0.1\n",
    "gamma = 0.01\n",
    "teacher_save_path = \"gcn_teacher.pth\"\n",
    "student_save_path = \"mlp_student.pth\"\n",
    "\n",
    "# --- Step 1: Train the Teacher (GCN) ---\n",
    "def train_teacher():\n",
    "    teacher_model.train()\n",
    "    optimizer_teacher.zero_grad()\n",
    "    out = teacher_model(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_teacher.step()\n",
    "\n",
    "def evaluate_teacher():\n",
    "    teacher_model.eval()\n",
    "    out = teacher_model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Training the teacher\n",
    "for epoch in range(num_epochs):\n",
    "    train_teacher()\n",
    "    acc = evaluate_teacher()\n",
    "    print(f'Epoch {epoch}, Teacher Model Accuracy: {acc:.4f}')\n",
    "\n",
    "# Save trained GCN teacher model\n",
    "torch.save(teacher_model.state_dict(), teacher_save_path)\n",
    "\n",
    "# --- Step 2: Distill Teacher Knowledge to Student (MLP) ---\n",
    "teacher_model.load_state_dict(torch.load(teacher_save_path))\n",
    "teacher_model.eval()\n",
    "\n",
    "def calc_orth_loss(out1, out2):\n",
    "    dot_product = (out1 * out2).sum(dim=1).pow(2)\n",
    "    norm1 = out1.norm(dim=1)\n",
    "    norm2 = out2.norm(dim=1)\n",
    "    orth_loss = ((dot_product / (norm1 * norm2).clamp(min=1e-6)).mean())\n",
    "    return orth_loss\n",
    "\n",
    "def train_student(soft_labels):\n",
    "    student_model.train()\n",
    "    optimizer_student.zero_grad()\n",
    "    out, hiddens = student_model(data)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    h1, h1c, h1_low, h2, h2c, h2_low = hiddens\n",
    "    layer_loss1 = criterion(gcn_conv(h1, data.edge_index), h1c) + criterion(gcn_conv(h2, data.edge_index), h2c)\n",
    "    layer_loss2 = criterion(gcn_conv_low_filter(h1, data.edge_index), h1_low) + criterion(gcn_conv_low_filter(h2, data.edge_index), h2_low)\n",
    "\n",
    "    orth_loss = calc_orth_loss(h1c, h1_low) + calc_orth_loss(h2c, h2_low)\n",
    "\n",
    "    # Get the output from the student model for the training nodes\n",
    "    student_output = out\n",
    "    \n",
    "    # Ensure the soft labels are only for the training nodes\n",
    "    teacher_soft_labels = soft_labels\n",
    "    \n",
    "    # KL divergence for soft targets (distillation loss)\n",
    "    loss_soft = F.kl_div(\n",
    "        F.log_softmax(student_output / temperature, dim=1),\n",
    "        teacher_soft_labels / temperature,\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    # Cross-entropy loss for ground truth labels\n",
    "    loss_hard = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    # Combined loss\n",
    "    loss = alpha * loss_soft + (1 - alpha) * loss_hard + beta * (layer_loss1 + layer_loss2) + gamma * orth_loss\n",
    "    loss.backward()\n",
    "    optimizer_student.step()\n",
    "\n",
    "def evaluate_student():\n",
    "    student_model.eval()\n",
    "    out, _ = student_model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Distillation: Train student model (MLP) using teacher's soft targets\n",
    "for epoch in range(5000):\n",
    "    # Get soft labels from the teacher\n",
    "    with torch.no_grad():\n",
    "        teacher_logits = teacher_model(data)\n",
    "        soft_labels = F.softmax(teacher_logits / temperature, dim=1)  # Temperature scaling\n",
    "\n",
    "    # Train the student with soft labels and ground truth\n",
    "    train_student(soft_labels)\n",
    "    acc = evaluate_student()\n",
    "    print(f'Epoch {epoch}, Student Model Accuracy: {acc:.4f}')\n",
    "\n",
    "# Save trained MLP student model\n",
    "torch.save(student_model.state_dict(), student_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4fdc9-7e40-49f4-9b03-256eb6ebb60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
